{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [09:33<00:00, 16.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 400, overlap: int = 70, embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\", es_user: str = \"elastic\", es_password: str = \"elastic\"):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.es = Elasticsearch(\n",
    "            \"http://localhost:9200\",\n",
    "            basic_auth=(es_user, es_password)\n",
    "        )\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "        self.document_store = []  # Store document chunks with their embeddings\n",
    "    \n",
    "    def pdf_to_text(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file.\"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,;?!-]', '', text)\n",
    "        return text\n",
    "\n",
    "    def segment_text(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into manageable chunks with overlap.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.overlap):\n",
    "            chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "            if chunk:\n",
    "                chunks.append({\n",
    "                    'content': chunk,\n",
    "                    'start_index': i,\n",
    "                    'length': len(chunk)\n",
    "                })\n",
    "        return chunks\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for text chunks.\"\"\"\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def index_document(self, doc_id: str, title: str, chunks: List[Dict[str, str]], embeddings: np.ndarray):\n",
    "        \"\"\"Index document chunks in both Elasticsearch and FAISS.\"\"\"\n",
    "        # Create Elasticsearch index if it doesn't exist\n",
    "        if not self.es.indices.exists(index=\"fifa_laws\"):\n",
    "            self.es.indices.create(\n",
    "                index=\"fifa_laws\",\n",
    "                mappings={\n",
    "                    \"properties\": {\n",
    "                        \"title\": {\"type\": \"text\"},\n",
    "                        \"content\": {\"type\": \"text\"},\n",
    "                        \"start_index\": {\"type\": \"integer\"},\n",
    "                        \"length\": {\"type\": \"integer\"},\n",
    "                        \"chunk_id\": {\"type\": \"keyword\"}\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Index each chunk\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            chunk_id = f\"{doc_id}_{i}\"\n",
    "            \n",
    "            # Store in Elasticsearch\n",
    "            self.es.index(\n",
    "                index=\"fifa_laws\",\n",
    "                id=chunk_id,\n",
    "                document={\n",
    "                    \"title\": title,\n",
    "                    \"content\": chunk[\"content\"],\n",
    "                    \"start_index\": chunk[\"start_index\"],\n",
    "                    \"length\": chunk[\"length\"],\n",
    "                    \"chunk_id\": chunk_id\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Store in FAISS\n",
    "            self.index.add(embedding.reshape(1, -1))\n",
    "            \n",
    "            # Store metadata\n",
    "            self.document_store.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"title\": title,\n",
    "                \"content\": chunk[\"content\"]\n",
    "            })\n",
    "    \n",
    "    def process_document(self, pdf_path: str, doc_id: str, title: str):\n",
    "        \"\"\"Process a complete document from PDF to indexed chunks.\"\"\"\n",
    "        # Extract text from PDF\n",
    "        text = self.pdf_to_text(pdf_path)\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Segment into chunks\n",
    "        chunks = self.segment_text(cleaned_text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings([chunk[\"content\"] for chunk in chunks])\n",
    "        \n",
    "        # Index the chunks\n",
    "        self.index_document(doc_id, title, chunks, embeddings)\n",
    "        \n",
    "        return len(chunks)\n",
    "\n",
    "    def save_indexes(self, path: str):\n",
    "        \"\"\"Save FAISS index and document store to disk.\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        faiss.write_index(self.index, f\"{path}/fifa_laws.faiss\")\n",
    "        with open(f\"{path}/document_store.json\", 'w') as f:\n",
    "            json.dump(self.document_store, f)\n",
    "    \n",
    "    def load_indexes(self, path: str):\n",
    "        \"\"\"Load FAISS index and document store from disk.\"\"\"\n",
    "        self.index = faiss.read_index(f\"{path}/fifa_laws.faiss\")\n",
    "        with open(f\"{path}/document_store.json\", 'r') as f:\n",
    "            self.document_store = json.load(f)\n",
    "# already executed\n",
    "# Initialize the DocumentProcessor with Elasticsearch credentials\n",
    "doc_processor = DocumentProcessor(es_user=\"elastic\", es_password=\"elastic\")\n",
    "\n",
    "# # Process a document (replace 'path/to/your/document.pdf' with the actual path to your PDF file)\n",
    "doc_processor.process_document(pdf_path=r'C:\\Users\\mouni\\OneDrive\\Bureau\\Fifa_Law_Consultant\\data\\FIFA_Legal_HB_EN V7.pdf', doc_id='doc1', title='Document Title')\n",
    "\n",
    "# # Save the indexes to the specified directory\n",
    "doc_processor.save_indexes('./indexes_with_overlap_70_chunks_400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\AppData\\Local\\Temp\\ipykernel_11796\\3863996049.py:3: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  es = Elasticsearch(\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    http_auth=(\"elastic\", \"elastic\")\n",
    ")\n",
    "\n",
    "# Check if the connection is successful\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Could not connect to Elasticsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "from elasticsearch import Elasticsearch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test search results: [{'title': 'Document Title', 'content': 'Congress; c to nominate candidates for the FIFA presidency and the Council; d to participate in and cast their votes at all FIFA elections in accordance with the FIFA Governance Regulations; e to take part in competitions organised by FIFA; f to take part in FIFAs assistance and development programmes; and g to exercise all other rights arising from these Statutes and other regulations. 2. The exercise of these rights is subject to other provisions in these Statutes and the applicable regulations. 17 17 TABLE OF CONTENTS FIFA LEGAL HANDBOOOK TABLE OF CONTENTS FIFA STATUTES 2024 ED. 14. Member associations obligations 1. Member associations have the following obligations a to comply fully with the Statutes, regulations, directives and decisions of FIFA bodies at any time as well as the decisions of the Court of Arbitration for Sport CAS passed on appeal on the basis of article 49 paragraph 1 of the FIFA Statutes; b to take part in competitions organised by FIFA; c to pay their membership subscriptions; d to cause their own members to comply with the Statutes, regulations, directives and decisions of FIFA bodies; e to convene its supreme and legislative body at regular intervals, at least every two years; f to ratify statutes that are in accordance with the requirements laid down in these Statutes; g to create a referees committee that is directly subordinate to the member association; h to respect the Laws of the Game; i to manage their affairs independently and ensure that their own affairs are not influenced by any third parties in accordance with article 19 of these Statutes; j to prevent and fight against any kind of discrimination; k to promote the development of womens football and the full participation of women at all levels; and l to comply fully with all other duties arising from these Statutes and other regulations. 2. Violation of the above-mentioned obligations by any member association may lead to sanctions provided for in these Statutes. 3. Violations of paragraph 1 i may also lead to sanctions, even if the third-party influence was not the fault of the member association concerned. Each member association is responsible towards FIFA for any and all acts of the members of their bodies caused by the gross negligence or wilful misconduct of such members. 18 FIFA Statutes II. Membership 18 TABLE OF CONTENTS FIFA LEGAL HANDBOOOK TABLE OF CONTENTS FIFA STATUTES', 'score': 1.0}, {'chunk_id': 'doc1_14', 'title': 'Document Title', 'content': 'Congress; c to nominate candidates for the FIFA presidency and the Council; d to participate in and cast their votes at all FIFA elections in accordance with the FIFA Governance Regulations; e to take part in competitions organised by FIFA; f to take part in FIFAs assistance and development programmes; and g to exercise all other rights arising from these Statutes and other regulations. 2. The exercise of these rights is subject to other provisions in these Statutes and the applicable regulations. 17 17 TABLE OF CONTENTS FIFA LEGAL HANDBOOOK TABLE OF CONTENTS FIFA STATUTES 2024 ED. 14. Member associations obligations 1. Member associations have the following obligations a to comply fully with the Statutes, regulations, directives and decisions of FIFA bodies at any time as well as the decisions of the Court of Arbitration for Sport CAS passed on appeal on the basis of article 49 paragraph 1 of the FIFA Statutes; b to take part in competitions organised by FIFA; c to pay their membership subscriptions; d to cause their own members to comply with the Statutes, regulations, directives and decisions of FIFA bodies; e to convene its supreme and legislative body at regular intervals, at least every two years; f to ratify statutes that are in accordance with the requirements laid down in these Statutes; g to create a referees committee that is directly subordinate to the member association; h to respect the Laws of the Game; i to manage their affairs independently and ensure that their own affairs are not influenced by any third parties in accordance with article 19 of these Statutes; j to prevent and fight against any kind of discrimination; k to promote the development of womens football and the full participation of women at all levels; and l to comply fully with all other duties arising from these Statutes and other regulations. 2. Violation of the above-mentioned obligations by any member association may lead to sanctions provided for in these Statutes. 3. Violations of paragraph 1 i may also lead to sanctions, even if the third-party influence was not the fault of the member association concerned. Each member association is responsible towards FIFA for any and all acts of the members of their bodies caused by the gross negligence or wilful misconduct of such members. 18 FIFA Statutes II. Membership 18 TABLE OF CONTENTS FIFA LEGAL HANDBOOOK TABLE OF CONTENTS FIFA STATUTES', 'score': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict\n",
    "import uvicorn\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, es_user: str = \"elastic\", es_password: str = \"your_password\", embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.es = Elasticsearch(\n",
    "            \"http://localhost:9200\",\n",
    "            basic_auth=(es_user, es_password)\n",
    "        )\n",
    "        # Initialize the embedding model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "\n",
    "    def load_indexes(self, path: str):\n",
    "        \"\"\"Load FAISS index and document store from disk.\"\"\"\n",
    "        self.index = faiss.read_index(f\"{path}/fifa_laws.faiss\")\n",
    "        with open(f\"{path}/document_store.json\", 'r') as f:\n",
    "            self.document_store = json.load(f)\n",
    "\n",
    "    def elasticsearch_search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Perform keyword-based search using Elasticsearch.\"\"\"\n",
    "        response = self.es.search(\n",
    "            index=\"fifa_laws\",\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"content\", \"title\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                        \"tie_breaker\": 0.3,\n",
    "                        \"minimum_should_match\": \"80%\"\n",
    "                    }\n",
    "                },\n",
    "                \"size\": top_k\n",
    "            }\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            results.append({\n",
    "                \"title\": hit[\"_source\"][\"title\"],\n",
    "                \"content\": hit[\"_source\"][\"content\"],\n",
    "                \"score\": hit[\"_score\"]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def faiss_search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Perform vector-based search using FAISS.\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            result = self.document_store[idx]\n",
    "            result[\"score\"] = float(dist)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Combine results from both search methods.\"\"\"\n",
    "        # Get results from both methods\n",
    "        es_results = self.elasticsearch_search(query, top_k * 2)\n",
    "        faiss_results = self.faiss_search(query, top_k * 2)\n",
    "\n",
    "        # Normalize scores\n",
    "        max_es_score = max([res[\"score\"] for res in es_results], default=1)\n",
    "        max_faiss_score = max([res[\"score\"] for res in faiss_results], default=1)\n",
    "\n",
    "        for res in es_results:\n",
    "            res[\"score\"] /= max_es_score\n",
    "        for res in faiss_results:\n",
    "            res[\"score\"] /= max_faiss_score\n",
    "\n",
    "        # Combine and sort results\n",
    "        combined_results = es_results + faiss_results\n",
    "        combined_results = sorted(combined_results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        return combined_results[:top_k]\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_name: str = \"gpt2\", embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\", index_path: str = \"./indexes\"):\n",
    "        # Initialize the retriever\n",
    "        self.retriever = HybridRetriever(es_user=\"elastic\", es_password=\"your_password\", embedding_model=embedding_model)\n",
    "        self.retriever.load_indexes(index_path)\n",
    "        # Initialize the language model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.generator = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def generate_response(self, question: str) -> Dict:\n",
    "        # Perform hybrid search\n",
    "        search_results = self.retriever.hybrid_search(question)\n",
    "        \n",
    "        # Combine the context from the top search results\n",
    "        context = \" \".join([result[\"content\"] for result in search_results[:3]])\n",
    "        \n",
    "        # Generate a response using the language model\n",
    "        generated_text = self.generator(f\"Question: {question}\\nContext: {context}\\nAnswer:\", max_length=150, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        \n",
    "        # Extract the answer from the generated text\n",
    "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "        print(\"gpt2 answer\",answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": search_results\n",
    "        }\n",
    "\n",
    "# Initialize the FastAPI app and RAG pipeline\n",
    "app = FastAPI(title=\"FIFA Law Consultation API\")\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag_pipeline = RAGPipeline(index_path='./indexes_with_overlap_70_chunks_400')\n",
    "\n",
    "# Define the Query model for API input\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "# Define the Source model to represent the sources returned in the response\n",
    "class Source(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    relevance_score: float\n",
    "    chunk_id: str  # Add chunk_id to the Source model\n",
    "\n",
    "# Define the Response model to structure the response returned to the user\n",
    "class Response(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    sources: List[Source]\n",
    "    context: Optional[str] = None\n",
    "\n",
    "# Create a POST endpoint to handle chat queries and generate responses using the RAG pipeline\n",
    "@app.post(\"/api/chat\", response_model=Response)\n",
    "async def chat_endpoint(query: Query):\n",
    "    try:\n",
    "        print(\"ok\")\n",
    "        # Generate response using the RAG pipeline\n",
    "        result = rag_pipeline.generate_response(query.question)\n",
    "        print(result)\n",
    "        # Calculate confidence score (for now, using a placeholder value or method)\n",
    "        confidence = 0.9  # Replace this with your actual confidence scoring method\n",
    "        \n",
    "        # Construct the response model\n",
    "        sources = [\n",
    "            Source(\n",
    "                title=source[\"title\"],\n",
    "                content=source[\"content\"],\n",
    "                relevance_score=source[\"score\"],\n",
    "                chunk_id=source.get(\"chunk_id\", \"\")  # Include chunk_id in the response\n",
    "            ) for source in result.get(\"sources\", [])\n",
    "        ]\n",
    "        \n",
    "        # Return response\n",
    "        return Response(\n",
    "            answer=result[\"answer\"],\n",
    "            confidence=confidence,\n",
    "            sources=sources,\n",
    "            context=result[\"context\"] if confidence > 0.5 else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Create a POST endpoint to directly access the hybrid search results\n",
    "@app.post(\"/api/search\")\n",
    "async def search_endpoint(query: Query):\n",
    "    \"\"\"Endpoint to directly access the hybrid search results.\"\"\"\n",
    "    try:\n",
    "        retriever = rag_pipeline.retriever\n",
    "        results = retriever.hybrid_search(query.question)\n",
    "        \n",
    "        # Transform the search results into the required format\n",
    "        sources = [\n",
    "            Source(\n",
    "                title=source[\"title\"],\n",
    "                content=source[\"content\"],\n",
    "                relevance_score=source[\"score\"],\n",
    "                chunk_id=source.get(\"chunk_id\", \"\")  # Include chunk_id in the response\n",
    "            ) for source in results\n",
    "        ]\n",
    "        \n",
    "        return {\"results\": sources}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# To run the FastAPI app, use the following command from the terminal (commented out here):\n",
    "# uvicorn filename:app --reload\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test retriever functionality (optional)\n",
    "    retriever = HybridRetriever(es_user=\"elastic\", es_password=\"elastic\")\n",
    "    retriever.load_indexes('./indexes_with_overlap_70_chunks_400')\n",
    "    results = retriever.hybrid_search(\"What are the obligations of member associations according to FIFA statutes?\")\n",
    "    print(\"Test search results:\", results[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fifa_law_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
